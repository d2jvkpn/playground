networks:
  net: { name: ollama, driver: bridge, external: false }

services:
  ollama:
    container_name: ollama
    hostname: ollama
    image: ollama/ollama:0.5.13
    #image: ollama/ollama:0.5.13-rocm # AMD  ROCm
    deploy:
      resources:
        reservations:
          # cpus: "1"
          # memory: 4g
          devices:
          - driver: nvidia
            # device_ids: ['0', '3']
            # count: all
            count: 1
            capabilities: [gpu]
    #network_mode: bridge
    networks: [net]
    ports: ["127.0.0.1:11434:11434"]
    volumes:
    #- /ect/localtime:/ect/localtime:ro
    #- /etc/timezone:/etc/timezone:ro
    - ./data/ollama:/root/.ollama

  open-webui:
    container_name: open-webui
    hostname: open-webui
    image: ghcr.io/open-webui/open-webui:0.5-cuda
    deploy:
      resources:
        reservations:
          # cpus: "1"
          # memory: 4g
          devices:
          - driver: nvidia
            # device_ids: ['0', '3']
            # count: all
            count: 1
            capabilities: [gpu]
    depends_on: [ollama]
    networks: [net]
    extra_hosts:
    - host.docker.internal:host-gateway
    ports: ["127.0.0.1:3000:3000"]
    environment:
      PORT: 3000 # default=8080
      CORS_ALLOW_ORIGIN: "*"
      ENABLE_OPENAI_API: "false"
      OLLAMA_BASE_URL: http://ollama:11434

      #HF_ENDPOINT: https://hf-mirror.com
      # /app/backend/data/cache/embedding/models/models--sentence-transformers--all-MiniLM-L6-v2/
      RAG_EMBEDDING_MODEL_AUTO_UPDATE: "false"

      #?RAG_RERANKING_MODEL_AUTO_UPDATE: "false"
      #?WHISPER_MODEL_AUTO_UPDATE: "false"
    volumes:
    - ./data/open-webui:/app/backend/data
