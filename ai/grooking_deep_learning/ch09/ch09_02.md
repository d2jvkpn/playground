# Title
---
```meta
version: 0.1.0
authors: ["Jane Doe<jane.doe@noreply.local>"]
date: 1970-01-01
```

#### Ch01. 
1. activation functions
- relu:
lambda x: (x>=0) * x
- sigmod: yes vs no
- tanh:
lambda x: np.tanh(x)
- softmax: which one
```
def softmax(x):
    temp = np.exp(x)
    return temp / np.sum(temp, axis=1, keepdims=True)
```
