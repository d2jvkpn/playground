networks:
   net: { name: ollama, driver: bridge, external: false }

services:
  ollama:
    container_name: ollama
    hostname: ollama
    image: ollama/ollama:0.8.0
    #network_mode: bridge
    networks: [net]
    ports: ["127.0.0.1:11434:11434"]
    volumes:
    #- /ect/localtime:/ect/localtime:ro
    #- /etc/timezone:/etc/timezone:ro
    - ./data/ollama:/root/.ollama
    command:
    - serve
    # ollama serve llama3:8b

  open-webui:
    container_name: open-webui
    hostname: open-webui
    image: ghcr.io/open-webui/open-webui:0.6
    depends_on: [ollama]
    networks: [net]
    extra_hosts:
    - host.docker.internal:host-gateway
    ports: ["127.0.0.1:3000:3000"]
    environment:
      PORT: 3000 # default=8080
      CORS_ALLOW_ORIGIN: "*"
      ENABLE_OPENAI_API: "false"
      OLLAMA_BASE_URL: http://ollama:11434
      #HF_ENDPOINT: https://hf-mirror.com
      # /app/backend/data/cache/embedding/models/models--sentence-transformers--all-MiniLM-L6-v2/
      RAG_EMBEDDING_MODEL_AUTO_UPDATE: "false"

      #?RAG_RERANKING_MODEL_AUTO_UPDATE: "false"
      #?WHISPER_MODEL_AUTO_UPDATE: "false"
    volumes:
    - ./data/open-webui:/app/backend/data

  # Open WebUI:
  #   Admin Panel ->
  #   Settings ->
  #   Connections ->
  #   OpenAI API ->
  #   Add Connection(+): URL="http://pipelines:9099", Key="0p3n-w3bu!" ->
  #   Pipelines
  pipelines:
    container_name: pipelines
    image: ghcr.io/open-webui/pipelines:main
    depends_on: [open-webui]
    extra_hosts:
      host.docker.internal: host-gateway
    networks: [net]
    #ports: ["127.0.0.1:9099:9099"]
    #environment:
    #  PIPELINES_URLS: https://github.com/open-webui/pipelines/blob/main/examples/filters/detoxify_filter_pipeline.py
    volumes:
    - ./data/pipelines:/app/pipelines
