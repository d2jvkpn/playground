services:
  open-webui:
    container_name: open-webui
    hostname: open-webui
    image: ghcr.io/open-webui/open-webui:0.6
    #depends_on: [ollama]
    extra_hosts:
      host.docker.internal: host-gateway
    #networks: [net]
    network_mode: bridge
    ports: ["127.0.0.1:3000:3000"]
    environment:
      #OFFLINE_MODE: "true"
      #HF_HUB_OFFLINE: "1"
      RUN_MODE: production
      TZ: Asia/Shanghai
      PORT: "3000" # default=8080
      WEBUI_SECRET_KEY: xxxxxxxxxxxxxxxx
      CORS_ALLOW_ORIGIN: "*"
      ENABLE_OPENAI_API: "false"
      #OLLAMA_BASE_URL: http://ollama:11434
      #HF_ENDPOINT: https://hf-mirror.com

      #QDRANT_URL: http://127.0.0.1:6333
      #QDRANT_API_KEY: xxxx
      #QDRANT_ON_DISK: "false"
      #QDRANT_PREFER_GRPC: "true"
      #QDRANT_COLLECTION_PREFIX: open-webui

      RAG_EMBEDDING_MODEL: BAAI/bge-m3 # nomic-embed-text, paraphrase-multilingual-MiniLM-L12-v2
      # model_path=/app/backend/data/cache/embedding/models/models--BAAI--bge-m3
      # default_model=sentence-transformers/all-MiniLM-L6-v2
      RAG_TEXT_SPLITTER: token
      RAG_EMBEDDING_MODEL_AUTO_UPDATE: "false"

      RAG_RERANKING_MODEL: BAAI/bge-reranker-base
      # model_path=/app/backend/data/cache/embedding/models/models--BAAI--bge-reranker-base
      RAG_RERANKING_MODEL_TRUST_REMOTE_CODE: "false"
      RAG_RERANKING_MODEL_AUTO_UPDATE: "false"

      WHISPER_MODEL: Systran/faster-whisper-medium
      # options=[Systran/faster-whisper-small, Systran/faster-whisper-large-v3]
      # model_path=/app/backend/data/cache/whisper/models/models--Systran--faster-whisper-large-v3
      WHISPER_MODEL_AUTO_UPDATE: "false"
    volumes:
    - ./data/open-webui:/app/backend/data

  open-webui:
    container_name: open-webui
    hostname: open-webui
    image: ghcr.io/open-webui/open-webui:0.6-cuda
    deploy:
      resources:
        reservations:
          # cpus: "1"
          # memory: 4g
          devices:
          - driver: nvidia
            # device_ids: ['0', '3']
            # count: all
            count: 1
            capabilities: [gpu]
    depends_on: [ollama]
    networks: [net]
    extra_hosts:
    - host.docker.internal:host-gateway
    ports: ["127.0.0.1:3000:3000"]
    environment:
      PORT: 3000 # default=8080
      CORS_ALLOW_ORIGIN: "*"
      ENABLE_OPENAI_API: "false"
      OLLAMA_BASE_URL: http://ollama:11434
      #HF_ENDPOINT: https://hf-mirror.com
      # /app/backend/data/cache/embedding/models/models--sentence-transformers--all-MiniLM-L6-v2/
      #?RAG_EMBEDDING_MODEL_AUTO_UPDATE: "false"
      #?RAG_RERANKING_MODEL_AUTO_UPDATE: "false"
      #?WHISPER_MODEL_AUTO_UPDATE: "false"
      #QDRANT_URL=http://localhost:6333
      #QDRANT_API_KEY=  # 如果你启用了认证
      #QDRANT_COLLECTION_NAME=open-webui
      #EMBEDDING_MODEL=nomic-embed-text
    volumes:
    - ./data/open-webui:/app/backend/data

  # Open WebUI:
  #   Admin Panel ->
  #   Settings ->
  #   Connections ->
  #   OpenAI API ->
  #   Add Connection(+): URL="http://pipelines:9099", Key="0p3n-w3bu!" ->
  #   Pipelines
  pipelines:
    container_name: pipelines
    image: ghcr.io/open-webui/pipelines:main
    depends_on: [open-webui]
    extra_hosts:
      host.docker.internal: host-gateway
    #networks: [net]
    network_mode: bridge
    #ports: ["127.0.0.1:9099:9099"]
    #environment:
    #  PIPELINES_URLS: https://github.com/open-webui/pipelines/blob/main/examples/filters/detoxify_filter_pipeline.py
    volumes:
    - ./data/pipelines:/app/pipelines
